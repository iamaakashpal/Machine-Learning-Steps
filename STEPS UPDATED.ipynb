{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING STEPS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Exploratory Data Analysis\n",
    "\n",
    "https://www.youtube.com/playlist?list=PLZoTAELRMXVPQyArDHyQVjQxjj_YmEuO9\n",
    "\n",
    "STEP 1 : EDA { Exploratory Data Analysis }\n",
    "\n",
    "    i. Check Numerical Features.\n",
    "    ii. Check Categorial Feature.\n",
    "    iii. Check Missing Values.\n",
    "    iv. Check Outliers.\n",
    "    v. Data Cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature Engineering\n",
    "\n",
    "https://www.youtube.com/playlist?list=PLZoTAELRMXVPwYGE2PXD3x0bfKnR0cJjN\n",
    "\n",
    "STEP 1 : Handling the Missing Values\n",
    "\n",
    "    For Numerical Value :\n",
    "\n",
    "    i. Mean / Median\n",
    "    ii. Random Sample Imputation\n",
    "    iii. Capturing NAN Value with a new feature.\n",
    "    iv. End of Distribution Imputation.\n",
    "    v. Arbitrary Value Imputation (Min / Max) value.\n",
    "\n",
    "    For Categorical Value :\n",
    "\n",
    "    i. Mode (Frequent Category Imputation).\n",
    "    ii.  Capturing NAN Value with a new feature.\n",
    "\n",
    "STEP 2 : Handling Imbalanced Dataset\n",
    "\n",
    "    i. Under Sampling ( Best for small dataset ).\n",
    "    ii. Over Sampling ( Best for large dataset ).\n",
    "\n",
    "STEP 3 : Treating the Outliers\n",
    "\n",
    "    i. IQR.\n",
    "\n",
    "STEP 4 : Scaling down the Data.\n",
    "\n",
    "    i. Standardization.\n",
    "    ii. Normalization.\n",
    "    iii. Robust Scaler (Scaling To Median And Quantiles).\n",
    "    iv. Gaussian Transformation.\n",
    "        - Logarithmic Transformation.\n",
    "        - Reciprocal Transformation\n",
    "        - Square Root Transformation.\n",
    "        - Exponential Transformation.\n",
    "        - Box Cox Transformation.\n",
    "\n",
    "STEP 5 : Converting the Categorical Features into Numerical Features.\n",
    "\n",
    "    i. One Hot Encoding {Nominal}.\n",
    "    ii. One Hot Encoding with many category in a feature {Nominal}.\n",
    "    iii. Count or Frequency Encoding {Nominal}.\n",
    "    iv. Mean Encoding {Nominal}.\n",
    "    v. Probability Ratio Encoding {Nominal}.\n",
    "    vi. Ordinal Number Encoding {Ordinal}.\n",
    "    vii. Target Guided {Ordinal Encoding}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Feature Selection\n",
    "\n",
    "    i. Variance Threshold ( Unsupervised )\n",
    "    ii. Correlation\n",
    "    iii. Chi-Square\n",
    "    iv. Genetic Algorithm\n",
    "    v. K Neighbour\n",
    "    vi. Feature Importance {Extra Tree Classifier}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Building \n",
    "    --> Supervised Machine Learning\n",
    "    \n",
    "    --> 1. Regression\n",
    "        \n",
    "        i. Linear Regression -- Handle Outlier(Y) -- Required Feature Transformation(Y).\n",
    "        ii. Ridge and Lasso Regression -- Handle Outlier(Y) -- Required Feature Transformation(Y).\n",
    "        iii. Decision Tree Regressor -- Handle Outlier(N)\n",
    "        iv. Random Forest Regressor -- Handle Outlier(N)\n",
    "        v. XGBoost Regressor -- Handle Outlier(N)\n",
    "        vi. AdaBoost Regressor -- \n",
    "        vii. ANN --        Required Feature Transformation(Y)\n",
    "        viii. RNN --        Required Feature Transformation(Y)\n",
    "\n",
    "    --> 2. Classification\n",
    "\n",
    "        i. Logistic Regression -- Handle Outlier(Y) -- Required Feature Transformation(Y)\n",
    "        ii. Decision Tree Classifier -- Handle Outlier(N)\n",
    "        iii. Random Forest Classifier -- Handle Outlier(N)\n",
    "        iv. XGBoost Classifier -- Handle Outlier(N)\n",
    "        v. AdaBoost Classifier --\n",
    "        vi. ANN --      -- Required Feature Transformation(Y)\n",
    "        vii. CNN --      -- Required Feature Transformation(Y)\n",
    "        viii. Naive Baye's Classifier -- Handle Outlier(N) -- \n",
    "\n",
    "================================================\n",
    "\n",
    "        ii. Gradent Boosting -- Handle Outlier(N)\n",
    "        iii. SVM -- Handle Outlier(N)\n",
    "        iv. KNN -- Handle Outlier(N) -- Required Feature Transformation(Y)\n",
    "\n",
    "    --> Unsupervised Machine Learning\n",
    "\n",
    "    ---> 1. Clustering\n",
    "    ---> 2. Dimensionality Reduction\n",
    "\n",
    "================================================\n",
    "\n",
    "        i. KMeans -- Handle Outlier(Y) -- Required Feature Transformation(Y)\n",
    "        ii. DBScan -- Handle Outlier(Y)\n",
    "        iii. Hierical Clustering -- Handle Outlier(Y) -- Required Feature Transformation(Y)\n",
    "        iv. KNN Clustering\n",
    "        v. PCA -- Handle Outlier(Y)\n",
    "        vi. LDA\n",
    "        vii. Neural Network -- Handle Outlier(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model Deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3196968d684371006099b3d55edeef8ed90365227a30deaef86e5d4aa8519be0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
